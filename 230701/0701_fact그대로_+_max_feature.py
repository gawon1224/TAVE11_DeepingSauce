# -*- coding: utf-8 -*-
"""0701 fact그대로 + max feature

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ittXbovPkLydpbRIICJdNUtkBNRsk_U_
"""

from google.colab import drive
drive.mount('/content/drive')

"""# **1. Importing libraries**"""

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.pipeline import Pipeline
from xgboost import XGBClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier

from sklearn.metrics import accuracy_score
## for data
import pandas as pd

import seaborn as sns
import matplotlib.pyplot as plt

import warnings
warnings.filterwarnings(action='ignore')

from sklearn.metrics import classification_report, confusion_matrix
from sklearn.svm import LinearSVC

import nltk # 문장 토크나이저

# 영어 불용어 - 불용어 모아 놓은 리스트 다운로드해 제거
nltk.download('all')
nltk.download('wordnet')
nltk.download('stopwords')
from nltk.corpus import stopwords

from nltk.tokenize import word_tokenize # 토큰화
from nltk.stem.porter import PorterStemmer # 어근 동일화 <-> 이거 말고도 "Lancaster Stemmer"

# 표제어 추출
from nltk.stem import WordNetLemmatizer

# 정규표현 처리
import re

test = pd.read_csv('/content/drive/MyDrive/DACON_JudgmentPrediction/data/test_jm.csv')
train = pd.read_csv('/content/drive/MyDrive/DACON_JudgmentPrediction/data/train_jm.csv')

train.head(2)

test.head(2)

"""train, test split"""

import random

from sklearn.model_selection import train_test_split
train_df, val_df = train_test_split(train, test_size=0.2, random_state=42)
train_df = train_df.reset_index(drop=True)
val_df = val_df.reset_index(drop=True)

train_df['winner']=0
train_df['winner_frequency']=0
train_df['win_percentage']=0

for i in range(len(train_df)):
  if train_df['first_party_winner'][i]==1:
    train_df['winner'][i] = train_df['first_party'][i]
  else:
    train_df['winner'][i] = train_df['second_party'][i]

train_df['first_party_win_percent'] = 0
train_df['first_party_frequency'] = 0

for i in range(len(train_df)):
  train_df['first_party_frequency'][i] = len(train_df.loc[train_df['first_party'] == train_df['first_party'][i]]) + len(train_df.loc[train_df['second_party'] == train_df['first_party'][i]])

  if (len(train_df.loc[train_df['first_party'] == train_df['first_party'][i]]) + len(train_df.loc[train_df['second_party'] == train_df['first_party'][i]])) > 1:
    train_df['first_party_win_percent'][i]= len(train_df.loc[train_df['winner'] == train_df['first_party'][i]]) /(len(train_df.loc[train_df['first_party'] == train_df['first_party'][i]]) + len(train_df.loc[train_df['second_party'] == train_df['first_party'][i]]))
  else:
    train_df['first_party_win_percent'][i] = 0.5

train_df=train_df.drop(['winner',	'winner_frequency','first_party_frequency','win_percentage'],axis=1)

val_df_first_party = val_df['first_party']
train_df_first_party = train_df[['first_party','first_party_win_percent']]
train_df_first_party=train_df_first_party.drop_duplicates()
test_first_party_병합 = pd.merge(val_df_first_party, train_df_first_party, how='left')
test_first_party_병합 = test_first_party_병합.fillna(0.5)
val_df['first_party_win_percent'] = test_first_party_병합['first_party_win_percent']

train_df.columns

val_df.columns

def categorize_probability(x):
    if x == 0:
        return 0
    elif 0 < x <= 0.33:
        return 1
    elif 0.33 < x < 0.5:
        return 2
    elif x == 0.5:
        return 3
    elif 0.5 < x < 0.57:
        return 2
    elif 0.57 <= x < 0.75:
        return 4
    elif 0.75 <= x < 1:
        return 5
    elif x == 1:
        return 6
    else:
        return -1

train_df['승률범주형분류'] = 0

for i in range(len(train_df)):
  train_df['승률범주형분류'][i] = categorize_probability(train_df['first_party_win_percent'][i])

val_df['승률범주형분류'] = 0

for i in range(len(val_df)):
  val_df['승률범주형분류'][i] = categorize_probability(val_df['first_party_win_percent'][i])

from nltk.corpus import names #corpus=말뭉치,이름 관련 부분 다루기 위한 객체

# 영어 데이터 전처리 함수
stops = set(stopwords.words('english'))
ps = nltk.stem.porter.PorterStemmer()
all_names=set(names.words())
lem = nltk.stem.wordnet.WordNetLemmatizer()

def cleaning(str):
    replaceAll = str

    # 특수문자 및 기호 등 필요없는 문자 제거
    only_english = re.sub('[^a-zA-Z]', ' ', replaceAll)

    # 대소문자 모두 소문자로 통일
    no_capitals = only_english.lower().split()

    # 이름, 불용어(분석에 필요없는 토큰) 제거
    all_names=set(names.words())
    no_stops = [word for word in no_capitals if not word in all_names|stops]

    # 어근 추츨을 통한 텍스트 정규화 작업
    stemmer_words = [ps.stem(word) for word in no_stops]

    # 표제어 추출 Lemmatisation (convert the word into root word)
    lem_text = [lem.lemmatize(word) for word in stemmer_words]

    # back to string from list
    text = " ".join(lem_text)

    return text

from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer(max_features = 400)

def get_vector(vectorizer, df, train_mode):
    if train_mode:
        X_facts = vectorizer.fit_transform(df['fact_processing'])
    else:
        X_facts = vectorizer.transform(df['fact_processing'])

    X = np.concatenate([X_facts.todense()], axis=1)
    return X

# 데이터 클리닝
train_df["fact_processing"] = train_df["facts"].apply(cleaning)
#train_fact = train_df["fact_processing"]

val_df["fact_processing"] = val_df["facts"].apply(cleaning)
#test_fact = test_df["fact_processing"]

train_df['first_party_ner'].astype("category")
train_df['second_party_ner'].astype("category")
train_df['issued_area'].astype("category")

val_df['first_party_ner'].astype("category")
val_df['second_party_ner'].astype("category")
val_df['issued_area'].astype("category")

# 특정 범주형 변수(categorical variable)을 더미변수 생성
train_cat = pd.get_dummies(data = train_df[["first_party_ner","second_party_ner","issued_area"]])
val_cat = pd.get_dummies(data = val_df[["first_party_ner","second_party_ner","issued_area"]])

# 데이터 벡터화
train_fact = get_vector(vectorizer, train_df, True)
train_fact = np.asarray(train_fact)
train_fact = pd.DataFrame(data=train_fact)

val_fact = get_vector(vectorizer, val_df, False)
val_fact = np.asarray(val_fact)
val_fact = pd.DataFrame(data=val_fact)

train_num = train_df['승률범주형분류']
train_target = train_df[['first_party_winner']]

val_num = val_df['승률범주형분류']

train_fin = pd.concat([train_fact,train_cat,train_num,train_target],axis=1,join='inner')
val_fin = pd.concat([val_fact, val_cat, val_num],axis=1,join='inner')

X_train = train_fin.drop(columns=['first_party_winner'])
y_train = train_fin['first_party_winner']

X_test = val_fin

y_test = val_df['first_party_winner']

X_train.columns = X_train.columns.astype(str)
X_test.columns = X_test.columns.astype(str)

model = LogisticRegression()

model = LogisticRegression()
model.fit(X_train, y_train)

model.score(X_test,y_test)

pred = model.predict(X_test)
pd.DataFrame(pred).value_counts()

"""랜덤포레스트"""

model = RandomForestClassifier()
model.fit(X_train, y_train)

model.score(X_test,y_test)

"""KNeighborsClassifier(n_neighbors=3)"""

model = knn=KNeighborsClassifier(n_neighbors=7)
model.fit(X_train, y_train)

model.score(X_test,y_test)

"""CATBOOST"""

!pip install catboost

from catboost import CatBoostClassifier, Pool

counts = list(y_train.value_counts())
class_weight = [counts[1]/sum(counts), counts[0]/sum(counts)]
print("weight :", class_weight)

model = CatBoostClassifier(random_seed=42,class_weights=class_weight, verbose=0)
model.fit(X_train, y_train)

model.score(X_test,y_test)

pred = model.predict(X_test)
pd.DataFrame(pred).value_counts()

"""#**party 종류 넣을까말까 정하기**"""

